{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T20:00:25.277290Z",
     "start_time": "2022-05-20T20:00:24.839821Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T20:01:03.991508Z",
     "start_time": "2022-05-20T20:01:03.952613Z"
    }
   },
   "outputs": [],
   "source": [
    "disney_titles_df = pd.read_csv('../Data2/disneyplus_titles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T20:01:24.937150Z",
     "start_time": "2022-05-20T20:01:24.873783Z"
    }
   },
   "outputs": [],
   "source": [
    "disney_credits_df = pd.read_csv('../Data2/disneyplus_credits.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T20:01:48.375168Z",
     "start_time": "2022-05-20T20:01:48.289860Z"
    }
   },
   "outputs": [],
   "source": [
    "netflix_titles_df = pd.read_csv('../Data2/netflix_titles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T20:02:05.459639Z",
     "start_time": "2022-05-20T20:02:05.321855Z"
    }
   },
   "outputs": [],
   "source": [
    "netflix_credits_df = pd.read_csv('../Data2/netflix_credits.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T20:03:43.621347Z",
     "start_time": "2022-05-20T20:03:43.565495Z"
    }
   },
   "outputs": [],
   "source": [
    "paramount_titles_df = pd.read_csv('../Data2/paramount_title.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T20:03:59.733582Z",
     "start_time": "2022-05-20T20:03:59.650764Z"
    }
   },
   "outputs": [],
   "source": [
    "paramount_credits_df = pd.read_csv('../Data2/paramount_credits.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T20:04:31.534352Z",
     "start_time": "2022-05-20T20:04:31.426069Z"
    }
   },
   "outputs": [],
   "source": [
    "amazon_titles_df = pd.read_csv('../Data2/amazon_titles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T20:04:46.896454Z",
     "start_time": "2022-05-20T20:04:46.669032Z"
    }
   },
   "outputs": [],
   "source": [
    "amazon_credits_df = pd.read_csv('../Data2/amazon_credits.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-20T20:11:35.636220Z",
     "start_time": "2022-05-20T20:11:35.616268Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "merge() got multiple values for argument 'how'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-0fd0dea3c5bd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmovies_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdisney_titles_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparamount_titles_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnetflix_titles_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'outer'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: merge() got multiple values for argument 'how'"
     ]
    }
   ],
   "source": [
    "movies_df = disney_titles_df.merge(paramount_titles_df,netflix_titles_df, on='id', how='outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate `name` Column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, I took the `name` column and split each name into its own individual/separate column.  Each *new* column of names will include an assignment of either **1** or **0**, where 1 will indicate that this name is included in this particular movie `title` or 0 indicating that the name *is not* included in this movie `title`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flat_list2 = [item for sublist in new_list for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flat_list2 = list(set(flat_list2))\n",
    "#flat_list2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating for loop to loop over each genre, creating separate column for each genre, and assigning True or False\n",
    "#for name in flat_list2:\n",
    "  #  if name in movies_df['name']:\n",
    "   #     movies_df[name] = 1 \n",
    "   # else:\n",
    "   #     movies_df[name] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OneHotEncode the name column in movies_df\n",
    "# ohe_name = pd.get_dummies(movies_df.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate movies_df and the columns I just OHE\n",
    "# movies_df = pd.concat([movies_df, ohe_name], axis=1)\n",
    "# pd.set_option('display.max_columns', 50) # setting display columns to 50, since there are over 78k columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop name and role columns\n",
    "# movies_df = movies_df.drop(columns= ['name','role'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check to ensure both name and role columns have been dropped\n",
    "# movies_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use TextHero To Apply NLP (Natural Language Processing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Info About TextHero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Texthero package was utlized to clean and prepare review text for feature extraction. Texthero is a relatively new package that helps to streamline many natural language processing tools, and can be used to create custom cleaning pipelines for text data. As seen in this notebook I created a custom pipeline fucntion that can be used to clean any new review text data via the preprocessing applications of Texthero. Common functions include the removal of punctuation, stopwords, and excess whitepace. Texthero can also be ued to stem words within documents, at this time there is no lemmetization function within Texthero.\n",
    "\n",
    "Outside the scope of this project Texthero also has a plethora of other preprocessing functions including the remvoal of things like urls, digits, html tags, brackets, and diacritics (accents), as well as methods to drop entries with no content and fillnas within text. All of which can be conviently wrapped into custom pipelines or applied individualy. It also has a default .clean method. Texthero can also be used to apply vectorizors to the cleaned text, including count vectorizor (tf) and tf-idf. It also contains numerous dimensionality reduction techniques such as principle component anaylsis, kmeans clustering, mean shift clustering, and density based clustering (DBSCAN). Even further still, Texthero has several visualization tools that could prove usefull for NLP tasks such as wordclouds (using genism) and interactive scatterplots (using plotly). Many of these tools, such as tf-idf vectorizor are quite similar to their counterparts in other packages but currently lack the depth of customization that sklearn or other packages provide, such as being able to set n_gram ranges. As Texthero is further developed I believe it will prove to be a powerful tool in NLP based projects that utilize text data, already showing its prowess in the ability to create custom cleaning pipelines easily and efficiently. Please see the documentation and github for further exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def clean_review(df):\n",
    "#    custom_pipeline = [preprocessing.lowercase,\n",
    "#                  preprocessing.remove_punctuation,\n",
    "#                  preprocessing.remove_stopwords,\n",
    "#                  preprocessing.stem,\n",
    "#                  preprocessing.remove_whitespace]\n",
    "#    df['clean_text'] = hero.clean(df['reviewText'], custom_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_movies_df = movies_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_movies_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# movies_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x in test_movies_df:\n",
    "   #  test_new_movies_df = test_movies_df.loc[test_movies_df[x] == test_movies_df.groupby(['title'])[x].transform(max)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x in movies_df:\n",
    "    # new_movies_df = movies_df.loc[movies_df[x] == movies_df.groupby(['title'])[x].transform(max)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort_title = movies_df.groupby(['title']).vote_count.transform(max)\n",
    "# movies_clean = clean_columns.loc[clean_columns.vote_count == vote_count_max]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
